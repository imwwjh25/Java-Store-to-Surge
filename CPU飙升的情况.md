## 一、 第一步：紧急止损（先保服务可用，避免雪崩）



CPU 99% 大概率导致服务超时、卡顿，先止损再排查，防止问题扩散

1. 优先扩容 / 分流：若为分布式架构，立刻将故障节点从负载均衡（Nginx / 网关）摘除，避免流量打满；有扩容能力直接扩容节点承接流量
2. 临时限流降级：若无法摘除，快速开启接口限流（如 Sentinel/Resilience4j），对非核心接口直接降级，减少 CPU 消耗
3. 记录现场：止损前先执行 `top -b -n 1 > cpu_error_top.log` 保存当前进程状态，避免排查时现场丢失

## 二、 第二步：快速定位（从系统层面找到耗 CPU 的进程 + 线程）



核心：**系统工具逐层穿透，先定进程，再定线程，最后抓栈**，Linux 环境为主（生产环境主流），Windows 对应工具补充。

### 阶段 1：定位耗 CPU 的进程（top 命令）



1. 执行命令：

   ```
   top
   ```

   （实时查看系统 CPU 使用）

   - 关键指标：`%CPU`列（进程 CPU 使用率）、`PID`列（进程 ID）
   - 重点看：找到`%CPU`接近 100% 或远超其他进程的目标进程（Java 应用进程名通常是`java`，PID 记下来，比如`1234`）
   - 补充：按`P`键（大写），会按 CPU 使用率从高到低排序，更快定位

2. 补充命令（精准筛选）：`top -p 1234`（只查看目标 PID 进程，避免干扰）

### 阶段 2：定位进程内耗 CPU 的线程（top -Hp 命令）



找到目标进程后，穿透到线程层面，找到真正耗 CPU 的线程（Java 进程内多线程，大概率是个别线程导致 CPU 飙满）

1. 执行命令：

   ```
   top -Hp 1234
   ```

   （-Hp：查看指定进程的所有线程）

   - 关键指标：`%CPU`（线程 CPU 使用率）、`PID`列（**线程 ID，注意是系统线程 ID，十进制**）
   - 重点看：找到`%CPU`最高的线程 ID，比如`1250`，记下来
   - 补充：按`P`键排序，快速锁定 TOP1 线程

2. 关键操作：线程 ID 转十六进制（Java 栈日志用十六进制线程 ID）

   - 执行命令：`printf "%x\n" 1250`（十进制转十六进制，比如结果是`4e2`，记下来，后面抓栈要用）
   - 为什么转：Java 的线程栈（jstack）日志中，线程 ID 是十六进制格式，必须对应才能匹配

### 阶段 3：抓取线程栈（jstack 命令，Java 应用核心）



拿到十六进制线程 ID 后，抓 Java 线程栈，定位耗 CPU 的线程对应的业务代码

1. 执行命令：

   ```
   jstack 1234 > cpu_error_jstack.log
   ```

   （抓取目标进程的所有线程栈，保存到日志文件）

   - 补充：若线程栈刷新快，多抓几次：`jstack 1234 >> cpu_error_jstack.log`（追加写入），避免遗漏

2. 解析栈日志：找到目标线程（十六进制 ID）

   - 打开`cpu_error_jstack.log`，搜索刚才的十六进制线程 ID（如`4e2`）
   - 重点看：线程状态 + 调用栈，核心线程状态对应问题类型：
     - `RUNNABLE`：线程正在运行，大概率是此处代码耗 CPU（如死循环、复杂计算、频繁 GC）
     - `WAITING/TIMED_WAITING`：线程等待，基本不耗 CPU，排除
     - `BLOCKED`：线程阻塞，若大量出现可能是锁竞争，但一般不会导致 CPU 99%（阻塞是等锁，不占 CPU）

### 阶段 4：补充抓取 CPU 火焰图（进阶，快速定位代码路径）



若线程栈看不懂，用火焰图快速定位耗 CPU 的方法，效率更高

1. 安装工具：生产环境常用`perf`（Linux 自带，若没有则安装`yum install perf`）
2. 执行命令：`perf record -F 99 -p 1234 -g -- sleep 30`（采样 30 秒，-g 记录调用栈）
3. 生成火焰图：`perf script | stackcollapse-perf.pl | flamegraph.pl > cpu_flame.svg`
4. 查看：用浏览器打开`cpu_flame.svg`，**纵向是调用栈，横向是执行时间**，越宽的函数越耗 CPU，直接定位到具体方法

## 三、 第三步：精准排查（根因分类 + 对应排查方法）



结合第二步的线程栈 / 火焰图，对应 Java 后端常见 CPU 飙满根因，逐一排查，你的分布式 + 微服务场景重点关注前 5 类

### 类别 1：死循环 / 无限循环（最常见，CPU 直接拉满）



- 特征：线程栈状态`RUNNABLE`，且调用栈固定在某段代码，无变化；CPU 100% 且稳定
- 排查重点：
  1. 看线程栈中的方法：是否有`while(true)`、`for(;;)`等循环，且循环内无退出条件 / 退出条件不生效
  2. 业务场景：比如秒杀场景的库存判断、订单状态轮询、消息消费重试逻辑，是否出现逻辑漏洞导致无限循环
  3. 示例：比如判断条件`if(num == 0)`写成`if(num = 0)`，导致循环永远成立

### 类别 2：频繁 GC（新生代 GC 频繁 / Full GC 卡死，CPU 飙高）



- 特征：CPU 99%，且`top`中 Java 进程的`%CPU`忽高忽低；线程栈中多线程处于`GC task thread`状态

- 排查步骤（核心）：

  1. 先看 GC 状态：

     ```
     jstat -gc 1234 1000 10
     ```

     

     （每 1 秒打印 1 次 GC，共 10 次）

     - 关键指标：`YGC`（新生代 GC 次数）、`YGCT`（新生代 GC 耗时）、`FGC`（Full GC 次数）、`FGCT`（Full GC 耗时）
     - 异常判断：YGC 每秒多次、YGCT 占比高；或 FGC 频繁（几分钟一次），FGCT 耗时长

  2. 抓堆快照：`jmap -dump:format=b,file=cpu_error_heap.hprof 1234`（保存堆快照）

  3. 分析堆快照：用 MAT/JProfiler 打开，看是否有内存泄漏（如静态集合未清理、线程池核心线程持有大对象、缓存无过期）

- 微服务场景重点：缓存未设置过期（如本地缓存 Guava Cache 无最大容量）、订单 / 用户数据查询后未释放、分页查询漏传页码导致查全量数据

### 类别 3：锁竞争激烈（自旋锁 / 轻量级锁自旋，CPU 飙高）



- 特征：CPU 99%，线程栈中多线程处于`BLOCKED`或`RUNNABLE`（自旋状态），涉及`synchronized`/`ReentrantLock`相关方法
- 排查重点（贴合你关注的锁机制）：
  1. 线程栈看锁对象：是否多个线程争抢同一把锁（如全局锁、单例对象锁）
  2. 锁类型：synchronized 升级为轻量级锁后，高并发下自旋次数过多（JVM 自适应自旋），导致 CPU 空转；ReentrantLock 非公平锁自旋频繁
  3. 业务场景：秒杀下单、支付回调、库存扣减等高频场景，锁粒度太大（如锁整个类而非锁资源）

### 类别 4：高频计算 / 序列化（无 IO 阻塞，纯 CPU 操作）



- 特征：CPU 99%，线程栈处于业务计算 / 序列化方法，无 IO 相关调用（如无 jdbc、redis、http 调用）
- 排查重点：
  1. 计算类：大数据量排序（如千万级列表 sort）、复杂算法（如风控规则、价格计算）、循环遍历大数据集合
  2. 序列化类：高频 JSON 序列化 / 反序列化（如 Jackson/fastjson 处理大对象）、ProtoBuf 编解码，且无缓存
  3. 微服务场景：网关层频繁转发且对请求 / 响应做序列化、分布式追踪（SkyWalking/Zipkin）全链路采样导致高频计算

### 类别 5：流量突增 + 代码未优化（高并发压垮 CPU）



- 特征：CPU 随流量上涨而飙满，排查时能看到接口 QPS 远超平时（如秒杀开抢、活动上线）
- 排查重点：
  1. 先看流量：网关 / 监控平台（Prometheus/Grafana）看目标接口 QPS、并发数
  2. 代码瓶颈：是否有未优化的查询（如无索引 SQL、全表扫描）、未缓存的热点数据（如商品详情、活动规则）、循环调用下游接口
  3. 微服务链路：是否出现链路穿透（如缓存击穿→数据库压力→服务线程飙高→CPU 满）

### 类别 6：其他少见原因



- 线程泄露：线程池核心线程数无上限，或创建大量线程（如 new Thread ()），线程数远超 CPU 核心数，上下文切换频繁导致 CPU 高
  - 排查：`jstack 1234 | grep "java.lang.Thread.State" | wc -l`（统计线程数），远超 CPU 核心数（如 16 核 CPU 线程数超 200）则异常
- 第三方组件问题：如 Redis 客户端（Jedis/Lettuce）无限重连、MQ 客户端（RocketMQ/Kafka）消费异常循环、日志框架（Logback）异步日志阻塞导致 CPU 高
- 系统层面：如 CPU 亲和性配置错误、内核进程抢占 CPU、病毒 / 挖矿程序（生产环境需排查）

## 四、 第四步：验证修复 + 复盘（避免复发）



### 1. 验证修复



- 修复代码后，重启服务（或热更），执行`top`观察 CPU 是否回落
- 压测验证：模拟高并发场景，确认修复后 CPU 稳定，无飙高
- 监控告警：临时添加 CPU 使用率告警（如超过 80% 告警），观察 1-2 小时

### 2. 复盘优化（核心是避免复发，适配分布式架构）



1. 代码层面：
   - 循环必加退出条件，避免死循环；大数据量处理分批执行（如分批查询、分批更新）
   - 锁粒度最小化（如锁资源 ID 而非全局锁），高并发场景用分布式锁替代本地锁，减少自旋
   - 热点数据强制缓存（本地缓存 + 分布式缓存），设置过期 + 降级，避免穿透
2. 监控层面：
   - 系统监控：CPU 使用率、负载、进程 / 线程数（Prometheus+Grafana），设置阈值告警（如 CPU>85% 告警）
   - Java 监控：GC 次数 / 耗时、堆内存、线程数、锁竞争（SkyWalking/Arthas），异常时自动抓栈
3. 架构层面：
   - 限流降级：核心接口必加限流（如按 QPS / 并发数限流），非核心接口降级兜底
   - 熔断隔离：下游服务异常时熔断（如 Resilience4j），避免链路阻塞导致 CPU 高
   - 容量评估：秒杀 / 活动前做压测，评估 CPU 承载上限，提前扩容

## 五、 必备排查工具汇总（Java 后端生产必备）



### 系统工具（Linux）



- 进程定位：top、ps -ef | grep java
- 线程定位：top -Hp、printf（十进制转十六进制）
- 采样分析：perf（火焰图）

### Java 专属工具（JDK 自带，无需额外安装）



- 线程栈：jstack（定位线程状态 + 调用栈）
- GC 监控：jstat（实时看 GC 状态）
- 堆快照：jmap（分析内存泄漏）
- 线程数统计：jstack + grep + wc

### 进阶工具（高效排查）



- 在线排查：Arthas（无需重启，直接排查 CPU 高线程、反编译代码、查看方法耗时）
- 堆分析：MAT、JProfiler
- 链路追踪：SkyWalking、Pinpoint（排查分布式链路瓶颈）

## 六、 快速排查口诀（记牢，生产应急直接用）



1. 先止损，摘节点，限流降级保可用；
2. top 定进程，Hp 找线程，转十六进制；
3. jstack 抓栈，看状态找调用栈，火焰图补位；
4. 死循环、频繁 GC、锁竞争、高流量，四类根因逐一查；
5. 修代码，做验证，监控复盘防复发。

### 1. LSMTree 实现：核心是 “分层存储 + 合并排序”

LSMTree（Log-Structured Merge Tree，日志结构合并树）的核心思路是 **“先把写操作快速存起来，再后台慢慢整理”**，避免像 B + 树那样写操作时频繁修改磁盘上的有序结构。其实现主要分为 “内存层” 和 “磁盘层” 两部分，具体结构如下：

#### （1）核心分层结构

| 层级   | 名称               | 存储位置      | 数据结构                       | 特点                                                         |
| ------ | ------------------ | ------------- | ------------------------------ | ------------------------------------------------------------ |
| 内存层 | MemTable           | 内存          | 有序结构（如跳表、平衡二叉树） | 读写极快，容量小；有大小上限，满了就 “刷盘”                  |
| 磁盘层 | Immutable MemTable | 内存→磁盘过渡 | 同 MemTable                    | MemTable 满后变为只读，等待异步刷到磁盘                      |
| 磁盘层 | SSTable            | 磁盘          | 不可变的有序键值对文件         | 按 Key 排序，支持二分查找；多文件分层（Level 0, Level 1...） |

#### （2）核心操作流程

1. **写操作（插入 / 更新 / 删除）**：
    - 所有写请求先写 “WAL 日志”（防止内存崩溃丢失数据），再写入 MemTable。
    - 当 MemTable 达到预设大小（如 64MB），立即转为 Immutable MemTable（只读），同时新建一个空 MemTable 接收新写请求。
    - 后台线程异步将 Immutable MemTable 刷成磁盘上的 SSTable 文件，默认放入磁盘第一层（Level 0）。
2. **合并操作（Compaction）**：
    - 随着 Level 0 的 SSTable 增多（或某一层文件大小超标），后台触发 Compaction：
        - 取出某一层的部分 SSTable，与下一层中 Key 有重叠的 SSTable 合并。
        - 合并时按 Key 排序，去重（同一 Key 保留最新版本）、过滤无效数据（标记删除的 Key），生成新的 SSTable 放入下一层。
    - 目的是减少磁盘文件数量，保证每层 SSTable 的 Key 范围尽量不重叠，优化后续读性能。
3. **读操作**：
    - 按 “内存→磁盘浅层→磁盘深层” 的顺序查找：先查 MemTable，再查 Immutable MemTable，最后按 Level 0→Level 1→... 的顺序查磁盘 SSTable。
    - 查 SSTable 时，先读文件的 “索引块”（内存缓存），通过二分查找定位 Key 所在的 “数据块”，再读取数据块找具体值。

### 2. LSM 的写性能为什么好？：“写操作只做最简单的事”

LSMTree 的写性能远优于 B + 树等传统索引，核心原因是**最大化减少 “随机磁盘 IO”，用 “顺序 IO” 和 “内存操作” 承接写请求**，具体有三个关键点：

1. **写操作只碰内存，不修改磁盘旧数据**：
    - 所有写请求（插入 / 更新 / 删除）最终都只写入内存中的 MemTable（跳表等结构支持 O (log n) 的有序插入），内存操作的速度比磁盘快几个数量级。
    - 即使是 “更新 / 删除”，也不修改磁盘上已有的 SSTable（SSTable 不可变），而是直接在 MemTable 中写一条新的 “版本记录”（删除用 “墓碑标记” 表示），避免了 B + 树写操作时 “分裂节点”“移动磁盘数据” 的随机 IO 开销。
2. **刷盘操作是顺序 IO**：
    - Immutable MemTable 刷盘时，因为本身是有序结构，直接按 Key 顺序写入磁盘生成 SSTable，属于 “顺序写磁盘”。
    - 顺序 IO 的速度是随机 IO 的 100 倍以上（磁盘物理特性决定：磁头不用频繁移动），而 B + 树的写操作常伴随随机 IO（如修改非叶子节点、分裂节点后写多个磁盘块）。
3. **后台异步合并，不阻塞前台写**：
    - Compaction（合并 SSTable）是后台线程异步执行的，不影响前台的写请求。即使合并过程耗时，前台依然能快速将写请求写入 MemTable，不会出现 “写阻塞”。

### 3. LSMTree 的读性能怎么样？：“读比写复杂，需权衡优化”

LSMTree 的读性能**不如写性能突出，且比 B + 树差**，核心原因是 “读操作需要多层查找”，但可以通过优化手段改善，具体分 “原生问题” 和 “优化方案” 两部分：

#### （1）原生读性能的痛点

1. **多路径查找，IO 次数多**：
    - 读一个 Key 时，需要按 “MemTable→Immutable MemTable→Level 0→Level 1→...→Level N” 的顺序依次查找，直到找到 Key。
    - 尤其是 Level 0 的 SSTable 可能有 Key 重叠（因为刷盘顺序是随机的），可能需要读多个 Level 0 文件才能确认 Key 是否存在，导致磁盘 IO 次数增加。
2. **历史版本干扰**：
    - 同一 Key 可能在不同层级的 SSTable 中存在多个版本（如多次更新），读操作需要找到 “最新的有效版本”，可能需要遍历多个文件，进一步增加开销。

#### （2）关键优化手段（提升读性能）

实际应用中（如 LevelDB、RocksDB）会通过以下优化缓解读性能问题：

- **Block Cache**：将 SSTable 的 “索引块” 和 “热点数据块” 缓存到内存，减少磁盘 IO（大部分读请求命中缓存，无需读磁盘）。
- **Bloom Filter（布隆过滤器）**：每个 SSTable 对应一个布隆过滤器，提前判断 “某个 Key 是否在该 SSTable 中”，避免无意义的磁盘文件读取（比如某 Key 不在 Level 0 的某个文件中，直接跳过，不用读该文件）。
- **Level 分层优化**：Level 1 及以上的 SSTable 保证 Key 范围不重叠，只需二分查找定位到一个文件即可，减少深层级的 IO 次数。

#### （3）总结读性能

- **理想情况（热点 Key 命中缓存）**：读性能接近内存，很快。
- **最坏情况（冷 Key 未命中缓存，需遍历多层 SSTable）**：读性能较差，IO 次数多。
- **整体定位**：读性能 “可接受但需优化”，适合 “写多读少” 的场景（如日志存储、消息队列、时序数据库）；如果是 “读多写少” 场景，B + 树（如 MySQL 的 InnoDB）更有优势。


https://juejin.cn/post/7314207903413878795
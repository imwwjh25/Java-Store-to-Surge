## [快手java二面凉经](https://www.nowcoder.com/discuss/528946479495467008?sourceSSR=users)


### 一、泛型的上下界界定与使用场景



泛型的上下界（Upper Bound / Lower Bound）用于**限制泛型参数的类型范围**，核心是通过 `extends`（上界）和 `super`（下界）关键字实现，遵循 “PECS 原则”（Producer Extends, Consumer Super）。

#### 1. 上界（Upper Bound）：`<? extends T>`



- **界定规则**：泛型参数必须是 `T` 或 `T` 的子类（包括直接子类、间接子类）。

- **核心特点**：只能 “读取” 泛型对象（生产者），不能 “写入”（除了 `null`）—— 因为无法确定泛型参数的具体子类，写入可能导致类型不兼容。

- 语法示例 ：



  ```
  // 泛型参数必须是 Number 或其子类（Integer、Double 等）
  public static double sum(List<? extends Number> list) {
      double total = 0;
      for (Number num : list) {
          total += num.doubleValue(); // 只能读取（生产者）
      }
      // list.add(10); // 编译报错：无法确定 list 实际是 Integer 还是 Double 列表，写入风险
      return total;
  }
  ```



- **使用场景**：当需要**获取泛型对象的属性 / 方法**（仅读取，不修改集合 / 对象本身）时，用上界。比如：统计数字列表的总和、获取对象的共同属性等。

#### 2. 下界（Lower Bound）：`<? super T>`



- **界定规则**：泛型参数必须是 `T` 或 `T` 的父类（包括直接父类、间接父类，直到 `Object`）。

- **核心特点**：只能 “写入” 泛型对象（消费者），读取时只能返回 `Object` 类型 —— 因为确定泛型参数是 `T` 的父类，写入 `T` 或其子类是安全的，但读取无法确定具体类型。

- 语法示例 ：



  ```
  // 泛型参数必须是 Integer 或其父类（Number、Object）
  public static void addIntegers(List<? super Integer> list) {
      list.add(10); // 可以写入（消费者：写入 Integer 及其子类，安全）
      list.add(20);
      // Integer num = list.get(0); // 编译报错：读取只能返回 Object，需强制转换
      Object obj = list.get(0); // 允许，但类型不明确
  }
  ```



- **使用场景**：当需要**向泛型集合 / 对象中添加元素**（仅写入，不依赖读取的具体类型）时，用下界。比如：向集合中添加特定类型的元素、批量插入数据等。

#### 3. 无界通配符：`<?>`（补充，非上下界，但常用）



- **界定规则**：泛型参数可以是任意类型（等价于 `<? extends Object>`）。
- **核心特点**：只能读取（返回 `Object`），不能写入（除了 `null`），适用于 “仅遍历，不修改” 的场景（如打印任意类型的集合）。

#### 4. 核心原则：PECS



- **Producer Extends**：如果泛型对象是 “生产者”（提供数据，仅读取），用上界 `<? extends T>`；
- **Consumer Super**：如果泛型对象是 “消费者”（接收数据，仅写入），用下界 `<? super T>`。

#### 总结：什么时候用什么？



| 需求场景                  | 泛型通配符                 | 原因                                    |
| ------------------------- | -------------------------- | --------------------------------------- |
| 读取泛型对象的属性 / 方法 | `<? extends T>`            | 限制类型范围，确保能安全调用 `T` 的方法 |
| 向泛型集合添加元素        | `<? super T>`              | 确保写入的元素类型与集合兼容            |
| 仅遍历，不读不写          | `<?>`                      | 任意类型均可，简化代码                  |
| 既读又写                  | 不使用通配符（直接用 `T`） | 通配符会限制读写，需明确泛型参数        |

### 二、超大规模用户抖音视频去重方案（用户量亿级）



核心思路：**基于 “唯一标识 + 分布式存储 + 高效索引”**，平衡去重准确性、查询性能和存储成本，避免单机瓶颈。

#### 1. 第一步：生成视频唯一标识（去重的核心依据）



必须保证 “同一视频” 生成唯一 ID，“不同视频” 生成不同 ID，常用方案：

- 方案 1：视频内容哈希（首选） ：对视频的关键帧（避免纯音频 / 封面篡改）进行 MD5/SHA-1 哈希，生成固定长度的哈希值（如 128 位 MD5）作为```video_id```。


- 优化：抽取视频前 3 帧 + 中间 2 帧 + 最后 3 帧，合并后哈希（减少篡改视频的误判）；

- **方案 2：业务唯一标识**：若视频是用户上传（非搬运），可用 “用户 ID + 上传时间戳 + 随机数” 生成唯一 `video_id`（但无法解决搬运视频去重）；

- **方案 3：AI 特征提取**：对视频进行特征向量提取（如 CNN 模型），通过向量相似度判断是否为同一视频（解决 “改分辨率 / 加滤镜” 的去重，成本较高）。

#### 2. 第二步：分布式去重存储与索引（支撑亿级用户）



用户量极大时，需拆分存储和查询压力，采用 “分片 + 缓存 + 持久化” 架构：

- 核心存储：

    - 分片规则：按 `user_id` 哈希分片（同一用户的去重记录存同一分片，避免跨分片查询）；
    - 持久化：用 Redis Cluster（缓存热点数据）+ HBase/ClickHouse（持久化全量数据）；
        - Redis：存储用户最近 N 条刷过的视频 `video_id`（如最近 1000 条，过期时间 7 天），支持 O (1) 查询；
        - HBase/ClickHouse：存储用户历史所有刷过的视频（按 `user_id + video_id` 作为行键，支持海量数据高效插入和查询）。

- 索引优化：

    - 对 HBase 的 `user_id` 列建立二级索引，加速 “查询用户所有刷过的视频”；
    - Redis 中用 `Set` 结构存储用户刷过的 `video_id`（`key=user:{user_id}:viewed_videos`，`value=video_id 集合`），支持 `SISMEMBER` 快速判断是否已刷过。

#### 3. 第三步：去重流程（实时判断 + 异步同步）



- 实时判断（用户刷视频时）：

    1. 用户请求刷视频，推荐系统生成候选视频列表；
    2. 对每个候选视频，先查 Redis：用 `SISMEMBER user:{user_id}:viewed_videos video_id` 判断是否已刷过；
    3. 若 Redis 存在（已刷过），跳过该视频；若不存在，查 HBase（避免 Redis 缓存穿透）；
    4. 若 HBase 也不存在（未刷过），返回视频给用户，并异步将 `video_id` 写入 Redis 和 HBase。

- 异步同步（避免实时压力）：

    - 用消息队列（Kafka）接收 “用户已刷视频” 事件，消费者异步将 `video_id` 写入 Redis（设置过期时间）和 HBase（持久化）；
    - 定期对 Redis 数据与 HBase 同步（如每天凌晨），清理过期数据，保证一致性。

#### 4. 第四步：优化策略（平衡性能与成本）



- 存储优化：

    - Redis 只存最近 N 条（如 1000 条），减少内存占用；历史数据存 HBase（列式存储，压缩比高）；
    - 对 `video_id` 进行压缩（如用 Base64 缩短字符串长度），降低存储成本。

- 性能优化：

    - 分片扩容：当单个分片压力过大时，拆分 `user_id` 哈希分片，水平扩展；
    - 缓存预热：对活跃用户的去重记录提前加载到 Redis，减少 HBase 查询；
    - 批量处理：推荐系统批量查询候选视频的去重状态（Redis 管道 / Pipeline），减少网络开销。

- 准确性优化：

    - 处理视频篡改：结合 “内容哈希 + AI 特征” 双重校验，避免误判；
    - 避免漏判：用户删除 “已刷记录” 后，同步删除 Redis 和 HBase 中的对应条目。

#### 核心架构总结：



`用户刷视频` → 推荐系统生成候选视频 → Redis 快速判断（O (1)）→ 未命中则查 HBase → 返回未刷视频 → 异步写入 Redis+HBase → 消息队列保障可靠性。

### 三、全中国身份证集合大小与存储空间估算



#### 1. 身份证集合大小估算（核心：有效身份证数量）



身份证号码是 18 位，前 6 位是行政区划代码，中间 8 位是出生日期（YYYYMMDD），后 4 位是顺序码 + 校验码。但实际有效身份证数量远小于理论最大值，需考虑：

- **理论最大值**：18 位身份证，最后 1 位是校验码（固定算法），前 17 位可组合 → 约 10^17 个（显然不现实）；

- 实际有效数量：

    - 中国总人口约 14.1 亿（2023 年数据），每人对应 1 个有效身份证（排除重复、失效身份证）；
    - 考虑少量重复（如补换身份证但旧证未注销）、临时身份证（15 位，已逐步淘汰），估算有效身份证总数约 **14.5 亿个**（冗余 3% 左右）。

#### 2. 存储空间估算（按 “存储身份证号字符串” 计算）



- 单条身份证存储大小：

    - 身份证号是 18 位字符，若用 UTF-8 编码（1 个字符 = 1 字节），单条占用 18 字节；
    - 若用定长字符串存储（无额外开销），单条 18 字节；若用数据库（如 MySQL）存储，`VARCHAR(18)` 会额外占用 1-2 字节长度标识，单条约 20 字节。

- 总存储空间：

    - 基础计算（UTF-8 无额外开销）：14.5 亿 × 18 字节 = 261 亿字节 ≈ 249 GB；
    - 数据库存储（含长度标识）：14.5 亿 × 20 字节 = 290 亿字节 ≈ 275 GB；
    - 冗余存储（备份、索引）：实际生产中需 2-3 倍冗余，总存储空间约 **500 GB - 800 GB**。

#### 3. 优化存储方案（减少空间占用）



- **方案 1：数字类型存储**：身份证号前 17 位是数字，最后 1 位可能是 `X`（可映射为 10），用 `BigInteger` 或拆分存储（前 17 位用 `Long`，最后 1 位用 `Char`），单条可压缩到 10 字节左右，总空间约 145 GB；
- **方案 2：哈希存储**：若仅需去重 / 查询，可将身份证号哈希为 64 位 MD5（8 字节），总空间约 116 GB（但会有极小哈希冲突风险，需结合原号校验）。

#### 总结：



- 有效身份证集合大小：约 **14.5 亿个**；
- 基础存储空间（UTF-8 字符串）：约 **250 GB**；
- 生产环境存储空间（含冗余 + 索引）：约 **500 GB - 800 GB**（按字符串存储），优化后可降至 200 GB 以内。

### 四、Linux 中查询 Tomcat 今天访问量最大的 IP / 页面（基于访问日志）



Tomcat 访问日志默认路径：`$CATALINA_HOME/logs/localhost_access_log.YYYY-MM-DD.txt`（按日期拆分），日志格式默认是 `Combined Log Format`（包含 IP、访问时间、请求路径等）。

#### 前提：确认日志格式（以默认格式为例）



日志每行格式：


```plaintext
192.168.1.1 - - [16/Nov/2025:10:00:00 +0800] "GET /index.html HTTP/1.1" 200 1024 "https://xxx.com" "Mozilla/5.0..."
```



字段含义：`IP地址 - - [访问时间] "请求方法 请求路径 协议" 状态码 字节数 "Referer" "User-Agent"`

#### 1. 需求 1：查询今天访问量最大的 IP



核心：提取今天日志中的所有 IP，统计出现次数，按降序排序。


```
# 步骤 1：进入 Tomcat 日志目录
cd $CATALINA_HOME/logs

# 步骤 2：提取今天的日志（假设今天是 2025-11-16，日志文件名为 localhost_access_log.2025-11-16.txt）
LOG_FILE="localhost_access_log.$(date +%Y-%m-%d).txt"

# 步骤 3：统计 IP 访问次数（取前 10 个）
awk '{print $1}' $LOG_FILE | sort | uniq -c | sort -nr | head -n 10
```



- 命令解释：
    - `awk '{print $1}'`：提取日志第 1 列（IP 地址）；
    - `sort`：排序 IP，便于 `uniq` 去重；
    - `uniq -c`：统计每个 IP 出现的次数（`-c` 表示计数）；
    - `sort -nr`：按次数降序排序（`-n` 数字排序，`-r` 反向）；
    - `head -n 10`：显示前 10 个访问量最大的 IP。

#### 2. 需求 2：查询今天访问量最大的页面（请求路径）



核心：提取日志中的请求路径，统计出现次数。



```
# 基于上面的 LOG_FILE 变量，提取请求路径（第 7 列）
awk '{print $7}' $LOG_FILE | sort | uniq -c | sort -nr | head -n 10
```



- 命令解释：`$7` 对应日志中的 “请求路径”（如 `/index.html`、`/api/user`）；

- 若需排除静态资源（.js/.css/.png 等），可添加过滤：

  ```
  awk '{print $7}' $LOG_FILE | grep -vE "\.(js|css|png|jpg|gif)$" | sort | uniq -c | sort -nr | head -n 10
  ```



- `grep -vE`：排除匹配正则的路径（静态资源后缀）。

#### 3. 需求 3：按时间段统计（如今天 10:00-12:00 访问量最大的 IP）



核心：先过滤指定时间段的日志，再统计 IP。


```
# 过滤今天 10:00-12:00 的日志（日期格式：[16/Nov/2025:10:xx:xx]）
awk '$4 ~ /\[16\/Nov\/2025:(10|11):/ {print $1}' $LOG_FILE | sort | uniq -c | sort -nr | head -n 10
```



- 命令解释：`$4 ~ /正则/` 表示匹配第 4 列（访问时间）中符合 “10 点或 11 点” 的记录。

#### 4. 注意事项



- 若日志文件较大（GB 级），可先用 `zcat` 解压压缩日志（如 `.gz` 文件）：`zcat $LOG_FILE.gz | awk ...`；
- 若 Tomcat 日志格式自定义（如字段顺序变化），需调整 `awk` 的列索引（如 `$1` 可能不是 IP，需根据实际格式修改）；
- 生产环境可结合 `ELK`（Elasticsearch+Logstash+Kibana）实时分析日志，无需手动执行命令。
Kafka 消息丢失可能发生在 **生产者发送、Broker 存储、消费者消费** 三个环节，需针对性设计保障机制，核心原则是 “确认机制 + 副本备份 + 故障恢复”：

#### 1. 生产者端：确保消息成功投递到 Broker



- 开启生产者确认（acks 参数） ：

    - `acks=0`：生产者发送后不等待 Broker 响应，可能丢失（不推荐）；
    - `acks=1`：仅 Leader 副本接收并写入日志后，向生产者返回成功（Leader 宕机可能丢失）；
    - `acks=-1/all`：Leader 接收后，需等待 **所有 ISR（In-Sync Replicas，同步副本）** 都写入日志，才返回成功（最可靠，推荐生产环境使用）。

- 开启生产者重试（retries 参数） ：

    - 当网络波动、Leader 选举等导致投递失败时，生产者自动重试（默认开启，可设置重试次数 `retries=N`）；
    - 配合 `retry.backoff.ms`（重试间隔）避免频繁重试，`max.in.flight.requests.per.connection=1`（限制单连接并发请求数）避免消息乱序。

- 使用事务 / 幂等性（可选，针对重复 + 丢失双重保障） ：

    - 幂等性生产者（`enable.idempotence=true`）：通过 PID + 序列号避免重复发送，同时确保重试时消息不丢失；
    - 事务生产者：将 “发送消息 + 提交偏移量” 包装为事务，确保原子性（适用于流处理等场景）。

#### 2. Broker 端：确保消息持久化 + 故障恢复



- 副本机制（多副本备份） ：

    - 每个 Topic 分区至少配置 2 个副本（`replication.factor ≥ 2`），包含 1 个 Leader 副本（处理读写）和 N-1 个 Follower 副本（同步 Leader 日志）；
    - 只有当 Follower 同步到 Leader 的最新消息后，才被纳入 ISR 列表（`min.insync.replicas ≥ 2`，确保 ISR 中至少有 2 个副本，避免 Leader 单点故障）。

- 日志持久化 ：

    - 消息写入 Leader 后，立即持久化到磁盘（而非内存），即使 Broker 宕机，重启后可从磁盘恢复数据；
    - 可通过 `log.flush.interval.messages`（消息数阈值）或 `log.flush.interval.ms`（时间阈值）控制刷盘频率（默认异步刷盘，平衡性能和可靠性）。

- 故障自动恢复 ：

    - 当 Leader 宕机时，Controller 会从该分区的 ISR 中选举新 Leader（优先选同步进度最新的 Follower），确保服务不中断，数据不丢失。

#### 3. 消费者端：确保消息成功消费 + 偏移量正确提交



- 手动提交偏移量（推荐） ：

    - 关闭自动提交（`enable.auto.commit=false`），在消费者**成功处理消息后**，手动调用 `commitSync()`（同步提交）或 `commitAsync()`（异步提交）；
    - 避免 “消息已接收但未处理完成，偏移量已提交，消费者宕机导致消息丢失”。

- 处理消费重试 ：

    - 若消费失败（如业务异常），不提交偏移量，将消息放入重试队列或延迟队列，重试成功后再提交；
    - 配合死信队列（DLQ），处理无法重试的消息，避免阻塞消费流程。

- 避免长时间未提交偏移量 ：

    - 若消费者长时间不提交偏移量，Broker 会认为消费者故障，触发 Rebalance，可能导致重复消费，但不会丢失消息（需在消费端做幂等处理）。

#### 面试回答总结：



“Kafka 从生产、存储、消费三个环节防止消息丢失：生产者端开启 acks=-1 确保所有 ISR 副本确认，配合重试机制；Broker 端通过多副本备份（replication.factor≥2）和 ISR 同步机制，确保 Leader 故障时可从 Follower 恢复；消费者端关闭自动提交，手动在消息处理成功后提交偏移量。全链路通过‘确认机制 + 副本 + 持久化’实现可靠性保障。”
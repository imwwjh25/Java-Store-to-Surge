### 千万级 URL 去重方案：分层过滤与分布式架构

#### 一、核心挑战与方案选型

千万级 URL 去重需平衡**速度、内存、精度**三个维度：

- **内存瓶颈**：若用传统哈希表（如 Java 的`HashSet`），存储 1000 万 URL 约需 1.7GB 内存（每个 URL 按 40 字节计算），且随着 URL 增长线性膨胀；
- **性能要求**：需支持毫秒级判断 “URL 是否已存在”；
- **精度需求**：允许一定概率误判（如 1%），但需避免漏判。

#### 二、经典方案对比与适用场景

| 方案                    | 原理                                                         | 内存占用（1000 万 URL） | 速度               | 误判率              | 适用场景                                      |
| ----------------------- | ------------------------------------------------------------ | ----------------------- | ------------------ | ------------------- | --------------------------------------------- |
| **哈希表（HashSet）**   | 直接存储 URL 字符串，基于哈希表实现 O (1) 查找               | ~1.7GB                  | 最快（0.8ms / 次） | 0%                  | 中小规模（≤500 万）、精度要求极高的场景       |
| **布隆过滤器（BF）**    | 通过 k 个哈希函数映射到位图，用概率算法判断存在性            | ~168MB（1% 误判率）     | 较快（1.2ms / 次） | 1%~0.1%             | 千万级、内存敏感、可接受低误判的场景          |
| **分布式存储（Redis）** | 用 Redis 的 Set 或 Bloom 模块实现多机共享去重状态            | 按需扩展（集群化）      | 中速（网络耗时）   | 0%（Set）/ 1%（BF） | 分布式爬虫、多节点协作的场景                  |
| **SimHash+BF**          | 先通过 SimHash 生成语义指纹（相似 URL 指纹接近），再用 BF 去重 | 指纹 + BF 内存          | 中速               | 可控                | 需要过滤 “相似 URL”（如同一网页的不同参数版） |

#### 三、工程实践：三层过滤架构（推荐方案）

借鉴搜索引擎的 “三层过滤网” 思路，兼顾**速度、内存、精度**：











未命中未命中未重复更新更新新URL内存布隆过滤器磁盘布隆过滤器分布式SimHash集群存储系统



##### 1. 第一层：内存布隆过滤器（快速过滤）

- **作用**：拦截 80% 以上的重复 URL，减少后续存储压力；

- **配置**：预期插入量 = 1000 万，误判率 = 0.01（内存约 168MB）；

- **实现**：用 Guava 的`BloomFilter`或 Redis 的 Rebloom 模块；

- 代码示例 （Guava）：








  ```java
  BloomFilter<String> memoryBF = BloomFilter.create(
      Funnels.stringFunnel(Charset.defaultCharset()), 
      10_000_000, 
      0.01
  );
  ```



##### 2. 第二层：磁盘布隆过滤器（持久化过滤）

- **作用**：处理内存 BF 溢出的 URL，支持服务重启后恢复状态；
- **实现**：用`RocksDB`或`LevelDB`存储位图，定期与内存 BF 同步；
- **优化**：采用 “可扩展布隆过滤器”（Scalable Bloom Filter），自动扩容并降低误判率。

##### 3. 第三层：SimHash 集群（语义去重）

- **作用**：识别 “语义重复” 的 URL（如`https://a.com?id=1`和`https://a.com?id=2`可能是同一页面）；
- **原理**：对 URL 生成 64 位 SimHash 指纹，汉明距离≤3 视为重复；
- **分布式部署**：用 Kafka+Flink 实时计算指纹，再用分布式 BF 存储结果。

#### 四、分布式场景优化

若需多节点协作（如分布式爬虫），可结合以下策略：

##### 1. 分片策略

- **按 URL 哈希分片**：将 URL 按哈希值取模，路由到不同节点的 BF 或 Redis 实例，避免单点压力；
- **一致性哈希**：支持节点扩缩容时的平滑迁移。

##### 2. Redis 集群方案

- **数据结构**：用`Redis Set`（精确去重）或`Rebloom`（概率去重）；

- 命令示例 （Rebloom）：










  ```bash
  # 添加URL
  BF.ADD crawl_filter https://example.com
  # 检查URL是否存在
  BF.EXISTS crawl_filter https://example.com
  ```



#### 五、参数计算与资源预估

以 1000 万 URL、1% 误判率为例：

- **布隆过滤器内存**：公式 \(m = -\frac{n \cdot \ln p}{(\ln 2)^2}\)，代入\(n=10^7\)、\(p=0.01\)，得\(m≈9.5×10^7\)位≈11.6MB（实际需预留冗余，约 168MB）；
- **机器配置**：单机 8GB 内存 + 4 核 CPU 可支撑，分布式场景建议 3 主 3 从 Redis 集群。

#### 六、方案取舍与建议

- **极致性能**：用 “内存 BF + 分布式 Redis BF”，牺牲 1% 误判率换毫秒级速度；
- **绝对精确**：用 “分布式 Redis Set + 数据库持久化”，但内存成本高；
- **语义去重**：在 BF 后加 SimHash 层，过滤 “参数不同但内容相同” 的 URL（如电商商品页）。